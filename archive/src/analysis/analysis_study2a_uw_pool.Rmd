---
title: "Untitled"
author: "Ashish"
date: "3/16/2021"
output: html_document
---

# Initialization
```{r}
rm(list=ls())
library(lavaan)
library(GGally)
library(tidyverse)
library(tidylog)
```

# Preprocessing
## Functions
```{r}
codebook_renamer <- function(df, codebook, old_col_name="old_col_name", new_col_name="new_col_name"){
  df %>% 
  rename_at(vars(codebook[[old_col_name]]), ~codebook[[new_col_name]])
}
recoder <- function(column, var_regex, var_col_name, codebook=codebook_values){
  codebook <- codebook %>%
    dplyr::filter(!is.na(old_value) | !is.na(new_value)) %>% 
    dplyr::mutate_at(vars(old_value, new_value), ~replace_na(., "<na_value>"))
  column <- column %>% 
    replace_na(., "<na_value>")
	level_key <- codebook %>%
	  dplyr::filter(.data[[var_col_name]]==!!var_regex) %>% 
	  dplyr::mutate(old_value = tolower(old_value)) %>% 
		dplyr::select(old_value, new_value) %>%
		deframe
	level_key <- c(level_key, "<na_value>"=NA)
	recoded_column <- recode(tolower(column), !!!level_key)
	 return(recoded_column)
}
codebook_recoder <- function(df, codebook, var_col_name="var_regex"){
  var_regexes <- unique(codebook[[var_col_name]])
  for(var in var_regexes){
    df <- df %>% 
      dplyr::mutate_at(vars(matches(paste0("^", var))), ~recoder(., var_regex=var, var_col_name=var_col_name))
  }
  return(df)
}
tally_scale <- function(df, scale_regex, new_col_name=NULL, pid_col="pid", join_function=dplyr::full_join, na.rm=T, tally_function=mean){
  if(is.null(new_col_name)) new_col_name <- scale_regex # if user did not supply a new column name, use the scale_regex
  scale_regex <- paste0("^", scale_regex) # append ^ to start of regex
  cols_to_tally <- names(df)[str_detect(names(df), scale_regex)] # identify columns to aggregate
  print(paste0("[",new_col_name, "] tallying columns:"))
  print(cols_to_tally)
  df_tally <- df %>% 
    dplyr::select(one_of(pid_col), one_of(cols_to_tally)) %>% # select columns to aggregate
    tidyr::pivot_longer(-one_of(pid_col)) %>% # convert to long format 
    dplyr::group_by_at(pid_col) %>% # group by participant
    dplyr::mutate(value = as.numeric(value)) %>% # change values to numeric
    dplyr::summarize(!!new_col_name := tally_function(value, na.rm=na.rm)) # aggregate according to tally_function
  if(!is.null(join_function)){
    df_tally <- df %>%
      join_function(df_tally) # join aggregated scores with original dataframe using join_function
  }
  return(df_tally)
}

  
flag_outliers <- function(x, na.rm = TRUE) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- rep(FALSE, times=length(x))
  y[x < (qnt[1] - H)] <- TRUE
  y[x > (qnt[2] + H)] <- TRUE
  y
}
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}
```

## Constants
```{r}
CODEBOOK_FILEPATH <- "doc/Codebook_Study_2a_Psych_Pool_Winter2021_EWB_Shift_Persist.xlsx"
DATA_FILEPATH <- "data/raw/Study_2A_UW_Psych_Pool_EWB_GB.xlsx"

COLS_TO_NUMERIC <- c("twn", "freq")
```


## Read data
```{r}
codebook_vars <- readxl::read_excel(CODEBOOK_FILEPATH, sheet="variables")
codebook_values <- readxl::read_excel(CODEBOOK_FILEPATH, sheet="values")

df_file <- readxl::read_excel(DATA_FILEPATH) %>% 
  slice(-1)

df_raw <- df_file %>% 
  # -- rename columns -- #
  codebook_renamer(codebook=codebook_vars) %>% 
  # -- add 4 digit pid -- #
  mutate(pid = 1000:(999+nrow(.))) %>% 
	# -- recode variables -- #
	codebook_recoder(codebook=codebook_values) 

```


## Process data
```{r}

df_proc <- df_raw %>% 
  # -- ER strategies -- #
  tally_scale("erq_reap", "erq_reap_mean") %>% 
  tally_scale("erq_supp", "erq_supp_mean") %>% 
  tally_scale("erqdf_", "erqdf_mean") %>% 
  tally_scale("ss_approach_.*(?<!mean)$", "ss_approach_mean") %>% 
  tally_scale("ss_avoid_.*(?<!mean)$", "ss_avoid_mean") %>% 
  tally_scale("ss_.*(?<!mean)$", "ss_mean") %>% 
  tally_scale("rs_", "rs_mean") %>% 
  # -- affective outcomes -- #
  # pos
  tally_scale( "swl_", "swl_mean") %>% 
  tally_scale( "ewb_", "ewb_mean") %>% 
  tally_scale( "fm_", "fm_mean") %>% 
  tally_scale( "panas_pa_", "panas_pa_mean") %>% 
  # neg
  tally_scale( "panas_na_", "panas_na_mean") %>% 
  tally_scale( "pss_", "pss_mean") %>% 
  # -- convert to numeric -- #
  mutate_at(vars(one_of(COLS_TO_NUMERIC)), as.numeric) %>%
  # -- drop outliers -- #
  mutate(twn_nooutliers = remove_outliers(twn)) 
  

```


# END PREPROCESSING
```{r}
# ------ End Preprocessing ------ #
# ----- Run all chunks above -----#
```

# Analysis

```{r}
df_proc %>% 
  ggplot(aes(x = twn)) + 
  geom_histogram()
df_proc %>% 
  ggplot(aes(x = freq)) + 
  geom_histogram()
```


1A. We hypothesize that nature contact will be associated with greater frequency of:
(i) situation selection in the form of approaching positively valenced situations, 
(ii) distraction, and 
(iii) reappraisal strategies and 
(iv) lesser frequency of rumination. 

Moreover, we hypothesize that emotion regulation strategies will be related to affective outcomes.

1B. We hypothesize that frequency of distraction, reappraisal, and rumination will mediate the association of nature contact with affective outcomes.

2A. We hypothesize that nature contact will moderate the associations of (i) adverse life experiences and (ii) perceived social status with affective outcomes.

2B. We hypothesize that these moderations will be mediated by frequency of reappraisal and rumination.

## H1a
Strategies associated with TWN: reappraisal, rumination (marginal)
Outcomes associated with TWN: Neg affect, flourishing (marginal), swl
```{r}
df_proc %>% 
  select(twn,twn_nooutliers, freq,
         ss_approach_mean, ss_avoid_mean, ss_mean,
         erq_reap_mean, erq_supp_mean, erqdf_mean, rs_mean,
         swl_mean, ewb_mean, fm_mean, panas_pa_mean,
         panas_na_mean, pss_mean) %>% 
  ggpairs()
cor.test(df_proc$freq, df_proc$ss_approach_mean, method="spearman")
cor.test(df_proc$freq, df_proc$ss_avoid_mean, method="spearman")
cor.test(df_proc$freq, df_proc$ss_mean, method="spearman")
cor.test(df_proc$freq, df_proc$erq_reap_mean, method="spearman")
cor.test(df_proc$freq, df_proc$erq_supp_mean, method="spearman")
cor.test(df_proc$freq, df_proc$erqdf_mean, method="spearman")
cor.test(df_proc$freq, df_proc$rs_mean, method="spearman")
cor.test(df_proc$freq, df_proc$swl_mean, method="spearman")
cor.test(df_proc$freq, df_proc$ewb_mean, method="spearman")
cor.test(df_proc$freq, df_proc$panas_pa_mean, method="spearman")
cor.test(df_proc$freq, df_proc$panas_na_mean, method="spearman")
cor.test(df_proc$freq, df_proc$pss_mean, method="spearman")
```

## H1b: twn -> reappraisal -> neg affect
```{r}
mod <- '
erq_reap_mean ~ a*twn
panas_na_mean ~ b*erq_reap_mean + cp*twn
indirect := a*b
total := indirect + cp
'
fit <- sem(mod, data=df_proc, se = "bootstrap")
summary(fit, standardized = TRUE)
parameterestimates(fit, boot.ci.type = "bca.simple", standardized = TRUE) 
```

## H1b:  twn -> reappraisal -> swl
```{r}
mod <- '
erq_reap_mean ~ a*twn
swl_mean ~ b*erq_reap_mean + cp*twn
indirect := a*b
total := indirect + cp
'
fit <- sem(mod, data=df_proc, se = "bootstrap")
summary(fit, standardized = TRUE)
parameterestimates(fit, boot.ci.type = "bca.simple", standardized = TRUE) 
```

## H1b:  twn -> reappraisal -> fm_mean
```{r}
mod <- '
erq_reap_mean ~ a*twn
fm_mean ~ b*erq_reap_mean + cp*twn
indirect := a*b
total := indirect + cp
'
fit <- sem(mod, data=df_proc, se = "bootstrap")
summary(fit, standardized = TRUE)
parameterestimates(fit, boot.ci.type = "bca.simple", standardized = TRUE) 
```

## H1b: twn -> rumination -> neg affect
```{r}
mod <- '
rs_mean ~ a*twn
panas_na_mean ~ b*rs_mean + cp*twn
indirect := a*b
total := indirect + cp
'
fit <- sem(mod, data=df_proc, se = "bootstrap")
summary(fit, standardized = TRUE)
parameterestimates(fit, boot.ci.type = "bca.simple", standardized = TRUE) 
```

## H1b:  twn -> rumination -> swl
```{r}
mod <- '
rs_mean ~ a*twn
swl_mean ~ b*rs_mean + cp*twn
indirect := a*b
total := indirect + cp
'
fit <- sem(mod, data=df_proc, se = "bootstrap")
summary(fit, standardized = TRUE)
parameterestimates(fit, boot.ci.type = "bca.simple", standardized = TRUE) 
```

## H1b:  twn -> rumination -> fm_mean
```{r}
mod <- '
rs_mean ~ a*twn
fm_mean ~ b*rs_mean + cp*twn
indirect := a*b
total := indirect + cp
'
fit <- sem(mod, data=df_proc, se = "bootstrap")
summary(fit, standardized = TRUE)
parameterestimates(fit, boot.ci.type = "bca.simple", standardized = TRUE) 
```

